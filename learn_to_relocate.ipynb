{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schelling and The Pursuit of Happyness\n",
    "\n",
    "In this notebook, we will try and propose a model that predicts when a relocation - in the realm of the Schelling model - is successiful.\n",
    "\n",
    "**NOTA**: the execution of this notebook implies running CPU-heavy and memory-heavy simulations and grid searches, that can be **extremely time-consuming**. However, after some synthetic data is generated, the notebook will save a checkpoint that can be retrieved easily.\n",
    "\n",
    "## Library installing\n",
    "\n",
    "For this task, we will need to install some libraries.\n",
    "- [MESA](https://mesa.readthedocs.io/en/stable/) is used for agent-based modelling.\n",
    "- [Scikit-Learn](https://scikit-learn.org/stable/) provides machine learning models and utility functions.\n",
    "- [Scikit-Optimize](https://scikit-optimize.github.io/) implements Bayesian optimization for Scikit-Learn.\n",
    "- [XGBoost](https://xgboost.readthedocs.io/en/stable/), [CatBoost](https://catboost.ai) and [LightGBM](https://lightgbm.readthedocs.io/en/v3.3.2/) contain two implementations of gradient boosted trees.\n",
    "- [imodels](https://github.com/csinva/imodels) contains implementations of some state-of-the art explainable models, most notably [FIGS](https://bair.berkeley.edu/blog/2022/06/30/figs/).\n",
    "\n",
    "We will also need [Pandas](https://pandas.pydata.org) to use the DataFrame data structure, [Matplotlib](https://matplotlib.org/) and [Graphviz](https://graphviz.org/) to plot some graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda install scikit-learn pandas matplotlib graphviz -y\n",
    "! conda install -c conda-forge mesa xgboost catboost lightgbm scikit-optimize -y\n",
    "! pip install imodels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MESA-based simulation\n",
    "\n",
    "We create a standard Schelling model with a mild-random policy, where when an agent is not happy it moves to a cell where its homophily is satisfied. The implementation can be found in `schelling/model.py` and `schelling/agent.py`.\n",
    "\n",
    "We run several simulations, with various combinations of parameters. In particular, we use:\n",
    "- `side`: Size of the grid. 8x8 (classical chessboard) and 20x20 (bigger, hence more complex dynamic may emerge)\n",
    "- `density`:  Density (percentage of the total cells that are populated). From 2% (grid is almost empty, hence the segregation should be faster) to 66% (two thirds of the cells are occupied).\n",
    "- `minority_pc`: Minority percentage (percentage of agents that are in the minority class):  From 1/4 (mild prevalence of the majority class) to 1/2 (equality between the two classes).\n",
    "- `homophily`: minimal fraction of the neighbors that an agent needs of its own type to be considered happy. From 1/8 (extremely tolerant) to 9/8 (extremely racist).\n",
    "\n",
    "For each parameter configuration, we store the `relocation_dataset` of each agent, updated each time an agent relocates from position `start` to position `end`, with the following features:\n",
    "- Agent-level features:\n",
    "    - `steps`: Number of steps passed in `start`.\n",
    "    - `relocations`: Number of previous relocations.\n",
    "    - `segregation`: Current value of the segregation at the moment of relocation.\n",
    "    - `segregation_min`, `segregation_max`, `segregation_avg`: Minimum, maximum and average segregation relative to the period passed in `start`.\n",
    "- Location-level features:\n",
    "    - `x`, `y`: Coordinates of the location.\n",
    "    - `density`: Number of occupied cells in the neighborhood of the location.\n",
    "    - `segregation_minority`, `segregation_majority`: Segregation of the minority/majority class in the location.`\n",
    "- Target label:\n",
    "    - `happyness_duration`: Fraction of the total number of simulation steps passed in `end` by the agent.\n",
    "\n",
    "In order to obtain a big number of records, we run every simulation 5 times, for a maximum of 500 iterations. For reproducibility, we set the random number generator with a fixed seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "parameters = {\n",
    "    \"side\": [10, 20],\n",
    "    \"density\": np.arange(2 / 100, 66 / 100, 1 / 5),\n",
    "    \"minority_pc\": np.arange(1 / 4, 1 / 2, 1 / 8),\n",
    "    \"homophily\": np.arange(1 / 8, 9 / 8, 1 / 8),\n",
    "}\n",
    "\n",
    "iterations = 5\n",
    "max_steps = 500\n",
    "random_seed = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the simulation, we use the MESA library's `batch_run` function.\n",
    "\n",
    "**NOTA**: The simulation is very time-consuming, so in the next cell it will be possible to download the pre-computed dataset from GitHub without the need of running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from mesa.batchrunner import batch_run\n",
    "\n",
    "from schelling.model import SchellingModel\n",
    "\n",
    "random.seed(random_seed)\n",
    "\n",
    "results = batch_run(\n",
    "    SchellingModel,\n",
    "    parameters=parameters,\n",
    "    number_processes=None,\n",
    "    iterations=iterations,\n",
    "    max_steps=max_steps,\n",
    "    display_progress=True\n",
    ")\n",
    "df = pd.concat((result[\"relocation_dataset\"] for result in results if result is not None), ignore_index=True)\n",
    "df.to_csv(\"./dataset.csv.gz\", compression=\"gzip\")\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation into a classification problem\n",
    "\n",
    "We want to discretize the `happyness_duration` target variable in order to solve a classification problem. By plotting an histogram of the variable discretized with 30 bins, we can observe how the distribution is very skewed, with a happyness duration that can be either very short (less than 1%) or very long (around 100%). In practice, this can be explained by the fact that a configuration can induce in the agent basically three behaviors:\n",
    "1) The agent is often unhappy and relocates continuously.\n",
    "2) The agent is happy after few relocations and stays for the whole run.\n",
    "3) \"Something in the middle\", with some relocations that are more successiful than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 3302455 records\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_url = \"https://github.com/karjudev/geospatial-analytics/blob/master/dataset.csv.gz?raw=true\"\n",
    "# Uncomment if you want to use a local copy\n",
    "# dataset_url = \"./dataset.csv.gz\"\n",
    "\n",
    "df = pd.read_csv(dataset_url, compression=\"gzip\")\n",
    "print(\"Dataset contains\", len(df), \"records\")\n",
    "\n",
    "df[\"happyness_duration\"].hist(bins=30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choices we can make to discretize the variables are basically two:\n",
    "\n",
    "- Equal-width binning: Find the threshold so that the range of values assigned to classes \"short\" and \"long\" are of the same width.\n",
    "- Equal-size binning: Find the threshold so that the ranges have approximately the same number of examples.\n",
    "\n",
    "Both strategies have pros and cons, so we try it in practice in order to better understand the impact of each on our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Equal width\")\n",
    "eq_width, eq_width_bins = pd.cut(df[\"happyness_duration\"], bins=2, labels=False, duplicates=\"drop\", retbins=True)\n",
    "display(eq_width_bins)\n",
    "display(eq_width.value_counts(normalize=True))\n",
    "\n",
    "print(\"Equal size\")\n",
    "eq_size, eq_size_bins = pd.qcut(df[\"happyness_duration\"], q=2, labels=False, duplicates=\"drop\", retbins=True)\n",
    "display(eq_size_bins)\n",
    "display(eq_size.value_counts(normalize=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With equal-size binning we have a more balanced distribution of positive and negative examples, but the threshold from \"short\" to \"long\" is fixed around 0.99. In this way we are assigning *all* the relocations that lasted less than 99% of the simulation steps to the negative class.\n",
    "\n",
    "For this reason, at the cost of having a slightly imbalanced dataset (53%-47% versus 50%-50%), we choose equal-width binning as our discretization strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"happyness_duration\"] = eq_width"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-validation-test split\n",
    "\n",
    "Let $\\mathbf{X}$ be the features matrix and $\\mathbf{y}$ be the target vector `happyness_duration`.\n",
    "\n",
    "Since we have many records, in order to speed up the training we sample from $\\mathbf{X}, \\mathbf{y}$ a *small* training set $\\mathbf{X}_\\mathrm{TR}, \\mathbf{y}_\\mathrm{TR}$ of $2^{16} = 65 536$ records, and we call development set $\\mathbf{X}_\\mathrm{DV}, \\mathbf{y}_\\mathrm{DV}$ the remaining rows. We use stratified sampling in order to keep the same proportion of examples per class.\n",
    "\n",
    "The training set will be used for model training and $K$-fold cross-validation, while the development set will be furtherly split into a validation set $\\mathbf{X}_\\mathrm{VL}, \\mathbf{y}_\\mathrm{VL}$ of $2^{16} = 65 536$ rows, used to evaluate the trained model on unseen data, and a test set $\\mathbf{X}_\\mathrm{TS}, \\mathbf{y}_\\mathrm{TS}$ with the rest, used in the end to obtain the final model's accuracy.\n",
    "\n",
    "We use the implementation of stratified sampling provided by the Scikit-Learn's `train_test_split` function. To preserve reproducibility, we use the same random seed as in the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "X = df.drop(columns=\"happyness_duration\").values\n",
    "y = df[\"happyness_duration\"].values\n",
    "\n",
    "sample_size = 2 ** 16\n",
    "\n",
    "random_seed = 0\n",
    "random.seed(random_seed)\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, train_size=sample_size, stratify=y, random_state=random_seed)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_dev, y_dev, train_size=sample_size, stratify=y_dev, random_state=random_seed)\n",
    "\n",
    "print(\"Training set:\", len(X_train), \"rows\")\n",
    "print(\"Validation set:\", len(X_val), \"rows\")\n",
    "print(\"Test set:\", len(X_test), \"rows\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset exploration\n",
    "\n",
    "We use Principal Component Analysis to visually explore the training set before starting the model selection. We select the first two principal component as axes, while the output class is represented with two different colors.\n",
    "\n",
    "We are able to spot a clear separation margin between the two classes, so the classification should be easy to achieve even with a simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit_transform(X_train)\n",
    "plt.scatter(x=X_r[:, 0], y=X_r[:, 1], c=y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification models\n",
    "\n",
    "We will benchmark a variety of machine learning models implemented by Scikit-Learn and imodels.\n",
    "\n",
    "Since we are dealing with tabular data, the literature and a simple rule of thumb suggests to put more effort investigating tree-based models. In a preliminary phase we also tried Support Vector Machine models, but they were unable to reach the baseline and were also very time-consuming to train.\n",
    "\n",
    "To perform an adequate model selection, we will use *Bayesian optimization* to find the optimal hyperparameter configuration. This method considers a function that takes the hyperparameter configuration, with parameters sampled from given distributions, and outputs the model's accuracy. Then it deploys global optimization to explore the search space and find the (approximate) optimum value for each hyperparameter. The algorithm is repeated 10 times, and a 4-fold cross-validation is used to assess the results with more robustness.\n",
    "\n",
    "To ease this, we write a simple utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "def k_fold_cv(model, hyperparameters, k=4, n_iter=10, random_seed=0):\n",
    "    clf = BayesSearchCV(model, hyperparameters, n_iter=n_iter, cv=k, n_jobs=-1, random_state=random_seed)\n",
    "    search = clf.fit(X_train, y_train)\n",
    "    return search.best_estimator_, search.best_params_, search.best_score_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a utility function also to evaluate a model's performances on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_accuracy(model, X_val, y_val):\n",
    "    y_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    return accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $K$-nearest neighbors\n",
    "\n",
    "The most simple, but powerful model we can test predicts the class of an instance with the weighted majority class among the $K$-nearest neighbor vectors in the training set. Such an elementary model can describe very complex dynamics, but some hyperparameters have to be tuned:\n",
    "- Size of the neighborhood, i.e. $K$: Number of neighbors to consider for each example.\n",
    "- Way to compute the weights associated to each neighbors: `uniform` means to consider each neighbor with the same weight, while `distance` means to assign a weight proportional to the distance from the example taken into account.\n",
    "- Leaf size: Number of examples that can be stored in a leaf in the specific data structure (BallTree or KDTree) used to represent the training set and speed up the queries.\n",
    "- $p$-norm to use: Wether to use $L_1$, $L_2$ or some other $L_p$ norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "hyperparams = {\n",
    "    \"n_neighbors\": (1, 10, \"uniform\"),\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "    \"leaf_size\": (2, 1024, \"uniform\"),\n",
    "    \"p\": [1, 2]\n",
    "}\n",
    "knn, knn_params, knn_score = k_fold_cv(model, hyperparams)\n",
    "print(\"Accuracy with K-fold CV:\", knn_score)\n",
    "print(\"Accuracy on the validation set:\", evaluate_accuracy(knn, X_val, y_val))\n",
    "print(\"Best configuration:\", knn_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of 99.865% is obtained with a leaf size of 153, $K = 8$ neighbors, $L_1$ norm and distance-based weighting of the neighborhood. This result is amazing and very difficult to surpass, but comes with the drawback that $K$-NN needs to store the full training set in memory and suffers the curse of dimensionality, i.e. more features lead to poorer results. For this reason we will proceed further in our exploration, keeping $K$-NN as our baseline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree\n",
    "\n",
    "A decision tree is a tree of rules automatically constructed to maximize some measure of the quantity of informations induceb by each rule. The hyparameters that we can effectively tweak are the following:\n",
    "- Criterion: The qualitative measure of the splitting induced by each rule. It can be set as the Gini coefficient, as the entropy or as the log loss.\n",
    "- Splitter: Rule used to select the split at each node. The algorithm can select the best split among all the possible ones or the best random split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=random_seed)\n",
    "hyperparams = {\n",
    "    \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    \"splitter\": [\"best\", \"random\"],\n",
    "}\n",
    "dt, dt_params, dt_score = k_fold_cv(model, hyperparams)\n",
    "print(\"Accuracy with K-fold CV:\", dt_score)\n",
    "print(\"Accuracy on the validation set:\", evaluate_accuracy(dt, X_val, y_val))\n",
    "print(\"Best configuration:\", dt_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the single decision tree is slightly lower that our baseline (accuracy 99.853%) maximizing the Gini coefficient with random splits.\n",
    "\n",
    "We can try and plot the tree to check its complexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "feature_names = df.drop(columns=\"happyness_duration\").columns.to_list()\n",
    "\n",
    "plt.figure(figsize=(80, 60), dpi=60)\n",
    "_ = plot_tree(dt, feature_names=feature_names, filled=True, fontsize=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model generated on this dataset is really complex, but in principle its predictions are explainable. We want also to plot the *importance* given by the model to the different features in terms of [mean decrease in impurity](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html), in order to better understand what features dominate the decisions. To do this, we write a simple utility function that plots a feature importance DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(importance_df, importance_name=\"Importance\", **kwargs):\n",
    "    importance_df.sort_values(by=importance_name, ascending=True, inplace=True)\n",
    "    importance_df.plot(kind=\"barh\", **kwargs)\n",
    "\n",
    "feat_importances = pd.DataFrame(dt.feature_importances_, index=feature_names, columns=[\"Importance\"])\n",
    "plot_feature_importance(feat_importances)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important feature of all is the number of relocations. This can be explained with the fact that if a user did many relocations in the past, the process of segregation is more likely to be already stabilized. It's the same at the start of the process, where an agent that performed few relocations is more likely to relocate again. Density in the `end` destination is also crucial, because a denser location (a location with a higher number of agents in the neighborhood) is more likely to be in an already segregated place.\n",
    "\n",
    "These performances are good, but we can reach a higher accuracy, at the cost of explainability, deploying two approaches: bagging and boosting of decision trees.\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "A random forest is a forest of decision trees trained on random subsets of the training set. At prediction time, the output class associated to an examples is the majority class among the ensemble of trees. This kind of models can perform very good on tabular data, at the expense of explainability of the overall decision model.\n",
    "\n",
    "The hyperparameters we need to tune are the following:\n",
    "- Number of estimators: Number of trees to build concurrently.\n",
    "- Criterion: As for the decision tree, whether to use Gini, entropy or log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_jobs=-1, random_state=random_seed)\n",
    "hyperparams = {\n",
    "    \"n_estimators\": (1, 200, \"uniform\"),\n",
    "    \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "}\n",
    "rf, rf_params, rf_score = k_fold_cv(model, hyperparams)\n",
    "print(\"Accuracy with K-fold CV:\", rf_score)\n",
    "print(\"Accuracy on the validation set:\", evaluate_accuracy(rf, X_val, y_val))\n",
    "print(\"Best configuration:\", rf_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ensemble model reaches the top performances so far (99.91% accuracy) with 157 trees built in parallel based on the measure of entropy.\n",
    "\n",
    "To have a quick peek on the complexity of the internal trees, we plot 5 random estimators from the ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ensemble_indices = random.sample(range(rf.n_estimators), 5)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(80, 20))\n",
    "for i, tree_id in enumerate(ensemble_indices):\n",
    "    _ = plot_tree(rf.estimators_[tree_id], filled=True, feature_names=feature_names, ax=axes[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the ensemble is constituted of very complex models, and randomization contributes to its very high accuracy.\n",
    "\n",
    "Let's plot a graph that synthetizes feature importance of the random forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.DataFrame(rf.feature_importances_, index=feature_names, columns=[\"Importance\"])\n",
    "plot_feature_importance(feat_importances)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the most important features are the number of relocations and the density in `end`, so the same reasoning used for the decision tree holds.\n",
    "\n",
    "### Gradient boosted trees\n",
    "\n",
    "Gradient Boosting is a technique in which a sequence of prediction models are generated one after the other, giving more weight to the examples that are mis-classified in the previous iteration. At prediction time, the predictions of all the models are weighted with respect to the precision. This leads to state-of-the art results for tabular data, but many hyperparameters have to be tuned.\n",
    "\n",
    "#### XGBoost\n",
    "\n",
    "This is the most famous implementation of gradient boosted trees. \n",
    "\n",
    "The hyperparameters we need to tune are:\n",
    "- Learning rate $\\eta$: Learning rate for the gradient boosting.\n",
    "- Maximum depth of the trees.\n",
    "- Minimum loss reduction $\\gamma$: Minimum reduction in the loss function needed to split a node.\n",
    "- Column sample: Percentage of columns sampled for each tree.\n",
    "- Regularization parameters $\\alpha$, relative to the $L_1$ norm of the weights, and $\\lambda$, relative to the $L_2$ norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier(verbosity=0, silent=True, random_seed=random_seed)\n",
    "hyperparams = {\n",
    "    \"learning_rate\": (0.0001, 1.0, \"log-uniform\") ,\n",
    "    \"max_depth\": (3, 21, \"uniform\"),\n",
    "    \"gamma\": (1e-16, 0.5, \"log-uniform\"),\n",
    "    \"colsample_bytree\": (3/10, 1.0, \"log-uniform\"),\n",
    "    \"reg_alpha\": (1e-5, 100.0, \"log-uniform\"),\n",
    "    \"reg_lambda\": (1e-5, 100.0, \"log-uniform\"),\n",
    "}\n",
    "xgb, xgb_params, xgb_score = k_fold_cv(model, hyperparams)\n",
    "print(\"Accuracy with K-fold CV:\", xgb_score)\n",
    "print(\"Accuracy on the validation set:\", evaluate_accuracy(xgb, X_val, y_val))\n",
    "print(\"Best configuration:\", xgb_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model does not surpas the random forest only for a 0.001%, reaching an accuracy of 99.914%. It used a column sample proportion of 33.5%, $\\gamma = 0.012$, learning rate $\\eta = 0.149$, maximum depth of 20, $\\alpha = 0.014$ and $\\lambda = 0.063$.\n",
    "\n",
    "Again, global explainability of the model is not easy to obtain, but as done for random forest, we can plot the a subsample of 5 estimators to inspect their complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_tree\n",
    "\n",
    "ensemble_indices = random.sample(range(xgb.n_estimators), 5)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(80, 60))\n",
    "\n",
    "for i, tree_id in enumerate(ensemble_indices):\n",
    "    plot_tree(xgb, num_trees=tree_id, ax=axes[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if the plot is not very clear, if we zoom we can clearly spot simpler models paired together with more complex models.\n",
    "\n",
    "As for the other models, we can plot the feature importance, this time with respect to various metrics:\n",
    "- `weight`: The number of times a feature is used for a split.\n",
    "- `gain`: Average gain across all splits a feature is used in.\n",
    "- `cover`: Average cover across all splits a feature is used in.\n",
    "- `total_gain`, `total_cover`: Same as `gain` and `cover`, but with total gain/cover instead of average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(60, 60))\n",
    "for i, importance_type in enumerate([\"weight\", \"gain\", \"cover\", \"total_gain\", \"total_cover\"]):\n",
    "    importances = xgb.get_booster().get_score(importance_type=importance_type)\n",
    "    feat_importances = pd.DataFrame(importances.values(), index=feature_names, columns=[\"Importance\"])\n",
    "    ax = plt.subplot(3, 2, i + 1)\n",
    "    plot_feature_importance(feat_importances, ax=ax, title=f\"Importance type: {importance_type}\", fontsize=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important feature in terms of information gain (average and total) is, as for the previous tree-based models, the number of relocations. Other criteria evidence how the `start` and `end` coordinates are highly discriminative features and are used in a variety of splits.\n",
    "\n",
    "### LightGBM\n",
    "\n",
    "This implementation of gradient-boosted trees by Microsoft differs from XGBoost in various ways, most importantly in the fact that trees in the forest grow leaf-wise instead of level-wise, leading to smaller and more controllable trees.\n",
    "\n",
    "The parameters to tune are more or less the same as the ones needed by XGBoost, but instead of the maximum depth of the trees we need to constraint the maximum number of leaves per tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model = LGBMClassifier(random_state=random_seed)\n",
    "hyperparams = {\n",
    "    \"n_estimators\": (1, 200, \"uniform\"),\n",
    "    \"learning_rate\": (0.0001, 1.0, \"log-uniform\") ,\n",
    "    \"num_leaves\": (20, 50, \"uniform\"),\n",
    "    \"colsample_bytree\": (3/10, 1.0, \"log-uniform\"),\n",
    "    \"reg_alpha\": (1e-5, 100.0, \"log-uniform\"),\n",
    "    \"reg_lambda\": (1e-5, 100.0, \"log-uniform\"),\n",
    "}\n",
    "lgb, lgb_params, lgb_score = k_fold_cv(model, hyperparams)\n",
    "print(\"Accuracy with K-fold CV:\", lgb_score)\n",
    "print(\"Accuracy on the validation set:\", evaluate_accuracy(lgb, X_val, y_val))\n",
    "print(\"Best configuration:\", lgb_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM is unable to reach the same performances as XGBoost, stopping at 99.90% accuracy on the validation set. Plotting 5 random estimators in this ensemble shows far less complex decision trees with respect to the ones of the random forest and XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import plot_tree\n",
    "\n",
    "ensemble_indices = random.sample(range(lgb.n_estimators), 5)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(80, 60))\n",
    "\n",
    "for i, tree_id in enumerate(ensemble_indices):\n",
    "    plot_tree(lgb, tree_index=tree_id, ax=axes[i], orientation=\"vertical\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot of feature importance shows that the most discriminative features are the coordinates and the segregation value in `start`. This suggests that our model may have found a way to distinguish a good relocation from a bad one not via a \"temporal\" attribute such as the number of previous relocations, but from the location-based features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.DataFrame(lgb.feature_importances_, index=feature_names, columns=[\"Importance\"])\n",
    "plot_feature_importance(feat_importances)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost\n",
    "\n",
    "This implementation by Yandex is the youngest of the three we proposed. It builds balanced trees, and uses *ordered boosting* in order to prevent overfitting, using a permutation-based approach to evaluate predictions on a different subset from the one used for training. It also natively supports all kind of features instead of using transformations to reduce all feature types to continuous values.\n",
    "\n",
    "It needs more or less the same hyperparameters needed to tune XGBoost and LightGBM, with a few exceptions:\n",
    "- Random strenght: Random component used to perturbate the scoring of different splits and avoid overfitting.\n",
    "- Bagging temperature: since CatBoost uses itself Bayesian optimization to find the optimal subset of features to train each tree on, it needs to find the optimal temperature value for this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "model = CatBoostClassifier(random_seed=random_seed, verbose=0)\n",
    "hyperparams = {\n",
    "    \"iterations\": Integer(10, 2000),\n",
    "    \"depth\": Integer(1, 12),\n",
    "    \"learning_rate\": Real(0.0001, 1.0, \"log-uniform\"),\n",
    "    \"random_strength\": Real(1e-9, 10, \"log-uniform\"),\n",
    "    \"bagging_temperature\": Real(0.0, 1.0),\n",
    "    \"l2_leaf_reg\": Integer(2, 100),\n",
    "}\n",
    "cb, cb_params, cb_score = k_fold_cv(model, hyperparams)\n",
    "print(\"Accuracy with K-fold CV:\", cb_score)\n",
    "print(\"Accuracy on the validation set:\", evaluate_accuracy(cb, X_val, y_val))\n",
    "print(\"Best configuration:\", cb_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost is not able to surpass XGBoost, stopping at 99.903% accuracy on the validation set. Since there is no readable way of visualizing some estimators in order to assess the model complexity, we skip this part and go directly to the feature importance bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.DataFrame(cb.feature_importances_, index=feature_names, columns=[\"Importance\"])\n",
    "plot_feature_importance(feat_importances)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost gives a higher feature importance to the coordinates of `start` and `end`, and immediatly after it ranks the number of relocations. This approach seems to take the best of both worlds from location-based and agent-based features.\n",
    "\n",
    "### Fast Incremental Greedy-tree Sums\n",
    "\n",
    "This is the newest tree-based model in the list. It has been proposed in 2022 by Singh et al. and it claims to reach the interpretability of CART (i.e. single decision tree) with the level of performance of gradient boosting trees.\n",
    "\n",
    "This is achieved by growing a family of trees simultaneusly. At each step, the algorithm grows an existing tree or create a new tree, so that the move minimizes the total unexplained variance. The prediction is constituted by the *sum* of the predictions of the single trees.\n",
    "\n",
    "This kind of architecture is very simple, and needs only one mandatory hyperparameter: the number of maximum rules per tree. Experiments show the model to be competitive with random forests and gradient boosted trees, often with a maximum number of rules $\\le$ 20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imodels import FIGSClassifier\n",
    "\n",
    "model = FIGSClassifier(random_state=random_seed)\n",
    "hyperparams = {\n",
    "    \"max_rules\": (1, 200, \"uniform\"),\n",
    "}\n",
    "\n",
    "figs, figs_params, figs_score = k_fold_cv(model, hyperparams)\n",
    "print(\"Accuracy with K-fold CV:\", figs_score)\n",
    "print(\"Accuracy on the validation set:\", evaluate_accuracy(figs, X_val, y_val))\n",
    "print(\"Best configuration:\", figs_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of 97% is far below the baseline, so for this problem FIGS is unable to generate a good model able to generalize. However, we have to note that the ensemble is constituted of only 3 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs.plot(feature_names=feature_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the best model\n",
    "\n",
    "Based on the evaluations on the validation test, we choose the XGBoost model as our go-to model for the `happyness_duration` prediction. We also test the model on the final, held-out test set, and obtain a very high accuracy of 99.91%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.save_model(\"./xgb.model\")\n",
    "\n",
    "accuracy = evaluate_accuracy(xgb, X_test, y_test)\n",
    "\n",
    "print(\"Accuracy on the test set:\", accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrinsic evaluation of the relocation policy\n",
    "\n",
    "We will use a subset of the parameters used to generate the training dataset, and this time we will perform the simulation with and without the trained relocation model.\n",
    "\n",
    "First, if needed, we will download from GitHub the model we previously found with the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-01-17 11:18:40--  https://github.com/karjudev/geospatial-analytics/blob/master/xgb.model?raw=true\n",
      "Risoluzione di github.com (github.com)... 140.82.121.4\n",
      "Connessione a github.com (github.com)|140.82.121.4|:443... connesso.\n",
      "Richiesta HTTP inviata, in attesa di risposta... 302 Found\n",
      "Posizione: https://github.com/karjudev/geospatial-analytics/raw/master/xgb.model [segue]\n",
      "--2023-01-17 11:18:41--  https://github.com/karjudev/geospatial-analytics/raw/master/xgb.model\n",
      "Riutilizzo della connessione esistente a github.com:443.\n",
      "Richiesta HTTP inviata, in attesa di risposta... 302 Found\n",
      "Posizione: https://raw.githubusercontent.com/karjudev/geospatial-analytics/master/xgb.model [segue]\n",
      "--2023-01-17 11:18:42--  https://raw.githubusercontent.com/karjudev/geospatial-analytics/master/xgb.model\n",
      "Risoluzione di raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connessione a raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connesso.\n",
      "Richiesta HTTP inviata, in attesa di risposta... 200 OK\n",
      "Lunghezza: 2281399 (2,2M) [application/octet-stream]\n",
      "Salvataggio in: ./xgb.model\n",
      "\n",
      "./xgb.model         100%[===================>]   2,17M  3,09MB/s    in 0,7s    \n",
      "\n",
      "2023-01-17 11:18:43 (3,09 MB/s) - ./xgb.model salvato [2281399/2281399]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karjudev/miniconda3/envs/gsa/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "! wget https://github.com/karjudev/geospatial-analytics/blob/master/xgb.model?raw=true -O ./xgb.model\n",
    "\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "relocation_model = XGBClassifier()\n",
    "relocation_model.load_model(\"./xgb.model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load a MESA simulation on the configurations used to generate the training set, but this time we will test mild random policy and the relocation policy that selects the free cell with the highest probability of happyness predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"side\": [10, 20],\n",
    "    \"density\": np.arange(2 / 100, 66 / 100, 1 / 5),\n",
    "    \"minority_pc\": np.arange(1 / 4, 1 / 2, 1 / 4),\n",
    "    \"homophily\": np.arange(1 / 8, 9 / 8, 1 / 4),\n",
    "    \"relocation_model\": [None, relocation_model]\n",
    "}\n",
    "\n",
    "iterations = 5\n",
    "max_steps = 500\n",
    "random_seed = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will collect the results of the simulation, then scan them to retrieve a set of records containing the number of steps needed to converge and the average `happyness_duration` of the relocations.\n",
    "\n",
    "If in the run's dataframe there are no relocations, there must be two cases:\n",
    "- There is only one step, i.e. all the agents are already happy. Hence the average happyness is 100%.\n",
    "- There is more than one step, hence there are no possible relocations at all that make the agent happy\n",
    "\n",
    "**NOTA**: The following simulation is very time-consuming, hence the result have been already pre-computed and cached on GitHub. There is no need to run the following cell in order to run the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from mesa.batchrunner import batch_run\n",
    "import pandas as pd\n",
    "\n",
    "from schelling.model import SchellingModel\n",
    "\n",
    "random.seed(random_seed)\n",
    "\n",
    "results = batch_run(\n",
    "    SchellingModel,\n",
    "    parameters=parameters,\n",
    "    number_processes=None,\n",
    "    iterations=iterations,\n",
    "    max_steps=max_steps,\n",
    "    display_progress=True\n",
    ")\n",
    "\n",
    "def analyze_result(result):\n",
    "    df = result[\"relocation_dataset\"]\n",
    "    if df is None or len(df) == 0:\n",
    "        # If there are no relocations, happyness duration can be 100% (every agent happy at first step)\n",
    "        # or 0% (no way to relocate in a happy place)\n",
    "        happyness_avg = 1.0 if result[\"steps\"] == 1 else 0.0\n",
    "        num_relocations = 0\n",
    "    else:\n",
    "        happyness_avg = df[\"happyness_duration\"].mean()\n",
    "        num_relocations = len(df)\n",
    "    return {\n",
    "        \"relocation_model\": result[\"relocation_model\"] is not None,\n",
    "        \"steps\": result[\"steps\"],\n",
    "        \"happyness_avg\": happyness_avg,\n",
    "        \"num_relocations\": num_relocations,\n",
    "    }\n",
    "\n",
    "records = [analyze_result(result) for result in results]\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "df.to_csv(\"./simulation.csv.gz\", compression=\"gzip\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explore the dataset with Pandas in order to drow our conclusions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>relocation_model</th>\n",
       "      <th>steps</th>\n",
       "      <th>happyness_avg</th>\n",
       "      <th>num_relocations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>315</td>\n",
       "      <td>False</td>\n",
       "      <td>501</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>316</td>\n",
       "      <td>True</td>\n",
       "      <td>501</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>317</td>\n",
       "      <td>True</td>\n",
       "      <td>501</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>318</td>\n",
       "      <td>False</td>\n",
       "      <td>501</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>319</td>\n",
       "      <td>True</td>\n",
       "      <td>501</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  relocation_model  steps  happyness_avg  num_relocations\n",
       "0             0             False    501            0.0                0\n",
       "1             1             False      2            1.0                7\n",
       "2             2             False    501            0.0                0\n",
       "3             3             False      2            1.0               17\n",
       "4             4             False    501            0.0                0\n",
       "..          ...               ...    ...            ...              ...\n",
       "315         315             False    501            1.0                3\n",
       "316         316              True    501            1.0                2\n",
       "317         317              True    501            1.0                5\n",
       "318         318             False    501            1.0                6\n",
       "319         319              True    501            1.0                2\n",
       "\n",
       "[320 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "simulation_url = \"https://github.com/karjudev/geospatial-analytics/blob/master/simulation.csv.gz?raw=true\"\n",
    "# Uncomment if you want to use a local copy\n",
    "# simulation_url = \"./simulation.csv.gz\"\n",
    "\n",
    "df = pd.read_csv(simulation_url, compression=\"gzip\")\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average convergence speed\n",
    "\n",
    "Plotting the average number of steps with and without the relocation model, we note that the average number of steps when using machine learning is in fact slightly lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.groupby(\"relocation_model\")[\"steps\"].mean().plot(\n",
    "    kind=\"bar\", xlabel=\"Relocation model\", ylabel=\"Average number of steps\"\n",
    ")\n",
    "ax.bar_label(ax.containers[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average happyness of the relocations\n",
    "\n",
    "The quantity we plot is the weighted average of the average happyness, so that it results in the average happyness of all the relocations. We clearly observe an improvement in the average happyness of around 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "def weighted_average(df, data_column, weight_column):\n",
    "    d = df[data_column]\n",
    "    w = df[weight_column]\n",
    "    return (d * w).sum() / w.sum()\n",
    "\n",
    "ax = df.groupby(\"relocation_model\").apply(weighted_average, \"happyness_avg\", \"num_relocations\").plot(\n",
    "    kind=\"bar\", xlabel=\"Relocation model\", ylabel=\"Average happyness of the relocations\"\n",
    ")\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0))\n",
    "ax.bar_label(ax.containers[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The research question we posed in the start was the following: can a machine learning model able to predict the success of a relocation speed up the convergence of a Schelling model?\n",
    "\n",
    "We generated a dataset, obtaining a fairly easy classification problem. To solve it, we compared a variety of machine learning models, and found one that gives particular importance to features relative to the agent and, in minor part, to the starting and end destination.\n",
    "\n",
    "We ran a simulation to assess the goodness of our approach, and in the end we stated that the answer is *positive*: it is possible to speed up the convergence with a properly constructed model. However, the improvement is fairly low, and the model needs a consistent amount of time and resources in order to build it. Hence, the effective value of this improvement is in fact arguable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea80f2394820fc6d34a9e4006290b8884c6d4fce86a3bee5fd2e83e111b37067"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
